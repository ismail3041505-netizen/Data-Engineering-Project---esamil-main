{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Web Scraper for Books.toscrape.com\n",
                "## ETL Project - TTTC3213\n",
                "\n",
                "This notebook extracts book data from the books.toscrape.com website.\n",
                "\n",
                "**Attributes scraped:**\n",
                "- Title\n",
                "- Price\n",
                "- Rating\n",
                "- Availability\n",
                "- Category\n",
                "- UPC\n",
                "- Description\n",
                "- Image URL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from bs4 import BeautifulSoup\n",
                "import pandas as pd\n",
                "import time\n",
                "import os\n",
                "from urllib.parse import urljoin"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration\n",
                "\n",
                "Define base URLs for the scraping operation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Base URL\n",
                "BASE_URL = \"https://books.toscrape.com/\"\n",
                "CATALOGUE_URL = \"https://books.toscrape.com/catalogue/\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Helper Functions\n",
                "\n",
                "Utility functions for fetching pages and converting data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_soup(url):\n",
                "    \"\"\"\n",
                "    Fetch a webpage and return a BeautifulSoup object.\n",
                "    Includes error handling and rate limiting.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        response = requests.get(url, timeout=10)\n",
                "        response.raise_for_status()\n",
                "        return BeautifulSoup(response.content, 'html.parser')\n",
                "    except requests.RequestException as e:\n",
                "        print(f\"Error fetching {url}: {e}\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def convert_rating_to_number(rating_class):\n",
                "    \"\"\"\n",
                "    Convert rating class name to numeric value.\n",
                "    Example: 'Three' -> 3\n",
                "    \"\"\"\n",
                "    rating_map = {\n",
                "        'One': 1,\n",
                "        'Two': 2,\n",
                "        'Three': 3,\n",
                "        'Four': 4,\n",
                "        'Five': 5\n",
                "    }\n",
                "    return rating_map.get(rating_class, 0)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Book Details Extraction\n",
                "\n",
                "Function to scrape detailed information from individual book pages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_book_details(book_url):\n",
                "    \"\"\"\n",
                "    Scrape detailed information from a book's individual page.\n",
                "    Returns: category, upc, description\n",
                "    \"\"\"\n",
                "    soup = get_soup(book_url)\n",
                "    if not soup:\n",
                "        return None, None, None\n",
                "    \n",
                "    # Get category from breadcrumb\n",
                "    breadcrumb = soup.find('ul', class_='breadcrumb')\n",
                "    category = None\n",
                "    if breadcrumb:\n",
                "        links = breadcrumb.find_all('a')\n",
                "        if len(links) >= 3:\n",
                "            category = links[2].text.strip()\n",
                "    \n",
                "    # Get UPC from product information table\n",
                "    upc = None\n",
                "    table = soup.find('table', class_='table-striped')\n",
                "    if table:\n",
                "        rows = table.find_all('tr')\n",
                "        for row in rows:\n",
                "            th = row.find('th')\n",
                "            td = row.find('td')\n",
                "            if th and td and th.text.strip() == 'UPC':\n",
                "                upc = td.text.strip()\n",
                "                break\n",
                "    \n",
                "    # Get description\n",
                "    description = None\n",
                "    desc_div = soup.find('div', id='product_description')\n",
                "    if desc_div:\n",
                "        desc_p = desc_div.find_next_sibling('p')\n",
                "        if desc_p:\n",
                "            description = desc_p.text.strip()\n",
                "    \n",
                "    return category, upc, description"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Main Scraping Function\n",
                "\n",
                "Core function that iterates through catalogue pages and extracts book data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scrape_books(max_pages=15):\n",
                "    \"\"\"\n",
                "    Main scraping function.\n",
                "    Scrapes book data from multiple pages of the catalogue.\n",
                "    \n",
                "    Args:\n",
                "        max_pages: Maximum number of pages to scrape (default 15 = 300 books)\n",
                "    \n",
                "    Returns:\n",
                "        List of dictionaries containing book data\n",
                "    \"\"\"\n",
                "    books = []\n",
                "    \n",
                "    for page_num in range(1, max_pages + 1):\n",
                "        # Construct page URL\n",
                "        if page_num == 1:\n",
                "            page_url = f\"{CATALOGUE_URL}page-1.html\"\n",
                "        else:\n",
                "            page_url = f\"{CATALOGUE_URL}page-{page_num}.html\"\n",
                "        \n",
                "        print(f\"Scraping page {page_num}: {page_url}\")\n",
                "        \n",
                "        soup = get_soup(page_url)\n",
                "        if not soup:\n",
                "            print(f\"Failed to fetch page {page_num}\")\n",
                "            continue\n",
                "        \n",
                "        # Find all book articles on the page\n",
                "        book_articles = soup.find_all('article', class_='product_pod')\n",
                "        \n",
                "        for article in book_articles:\n",
                "            try:\n",
                "                # Get title\n",
                "                title_tag = article.find('h3').find('a')\n",
                "                title = title_tag['title']\n",
                "                \n",
                "                # Get book detail page URL\n",
                "                book_relative_url = title_tag['href']\n",
                "                book_url = urljoin(page_url, book_relative_url)\n",
                "                \n",
                "                # Get price\n",
                "                price_tag = article.find('p', class_='price_color')\n",
                "                price = price_tag.text.strip() if price_tag else None\n",
                "                \n",
                "                # Get rating\n",
                "                rating_tag = article.find('p', class_='star-rating')\n",
                "                rating_class = None\n",
                "                if rating_tag:\n",
                "                    classes = rating_tag.get('class', [])\n",
                "                    for cls in classes:\n",
                "                        if cls != 'star-rating':\n",
                "                            rating_class = cls\n",
                "                            break\n",
                "                rating = convert_rating_to_number(rating_class)\n",
                "                \n",
                "                # Get availability\n",
                "                availability_tag = article.find('p', class_='instock')\n",
                "                availability = availability_tag.text.strip() if availability_tag else None\n",
                "                \n",
                "                # Get image URL\n",
                "                img_tag = article.find('img')\n",
                "                image_url = None\n",
                "                if img_tag:\n",
                "                    img_src = img_tag.get('src', '')\n",
                "                    image_url = urljoin(BASE_URL, img_src)\n",
                "                \n",
                "                # Get detailed information from book page\n",
                "                category, upc, description = get_book_details(book_url)\n",
                "                \n",
                "                # Create book record\n",
                "                book = {\n",
                "                    'title': title,\n",
                "                    'price': price,\n",
                "                    'rating': rating,\n",
                "                    'availability': availability,\n",
                "                    'category': category,\n",
                "                    'upc': upc,\n",
                "                    'description': description,\n",
                "                    'image_url': image_url,\n",
                "                    'book_url': book_url\n",
                "                }\n",
                "                \n",
                "                books.append(book)\n",
                "                print(f\"  Scraped: {title[:50]}...\")\n",
                "                \n",
                "                # Rate limiting - be respectful to the server\n",
                "                time.sleep(0.2)\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"Error scraping book: {e}\")\n",
                "                continue\n",
                "        \n",
                "        # Rate limiting between pages\n",
                "        time.sleep(0.5)\n",
                "    \n",
                "    return books"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save to CSV\n",
                "\n",
                "Function to save scraped data to a CSV file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def save_to_csv(books, filename):\n",
                "    \"\"\"\n",
                "    Save scraped books to a CSV file.\n",
                "    \"\"\"\n",
                "    df = pd.DataFrame(books)\n",
                "    \n",
                "    # Ensure data directory exists\n",
                "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
                "    \n",
                "    df.to_csv(filename, index=False, encoding='utf-8')\n",
                "    print(f\"\\nSaved {len(books)} books to {filename}\")\n",
                "    return df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Execute Scraping\n",
                "\n",
                "Run the main scraping workflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"Books.toscrape.com Web Scraper\")\n",
                "print(\"ETL Project - TTTC3213\")\n",
                "print(\"=\" * 60)\n",
                "print()\n",
                "\n",
                "# Scrape books (15 pages = 300 books max)\n",
                "print(\"Starting data extraction...\")\n",
                "books = scrape_books(max_pages=15)\n",
                "\n",
                "print(f\"\\nTotal books scraped: {len(books)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save and Display Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save raw data\n",
                "output_path = os.path.join(os.getcwd(), '..', 'data', 'raw_books.csv')\n",
                "output_path = os.path.abspath(output_path)\n",
                "\n",
                "df = save_to_csv(books, output_path)\n",
                "\n",
                "# Display sample\n",
                "print(\"\\nSample of scraped data:\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Data columns:\")\n",
                "print(df.columns.tolist())\n",
                "\n",
                "print(\"\\nData types:\")\n",
                "print(df.dtypes)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}